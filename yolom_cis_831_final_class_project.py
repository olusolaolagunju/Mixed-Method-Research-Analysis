# -*- coding: utf-8 -*-
"""YOLOm_CIS 831_FINAL_CLASS_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZRWAaFF7VJLu5aL5UcA3b-uq7-1_pkOo
"""

from google.colab import drive
drive.mount('/content/drive')

# Dont run again
from sklearn.model_selection import train_test_split
import json
import os
import shutil

# Paths
source_images_path = "/content/drive/MyDrive/wheat_merged/images"
train_images_folder = "/content/drive/MyDrive/wheat_merged/train/images"
val_images_folder = "/content/drive/MyDrive/wheat_merged/val/images"
test_images_folder = "/content/drive/MyDrive/wheat_merged/test/images"

# Create directories for train, val, and test images and their annotations subdirectories
os.makedirs(train_images_folder, exist_ok=True)
os.makedirs(val_images_folder, exist_ok=True)
os.makedirs(test_images_folder, exist_ok=True)
# Create the missing annotations directories
os.makedirs(os.path.join("/content/drive/MyDrive/wheat_merged/train", "annotations"), exist_ok=True)
os.makedirs(os.path.join("/content/drive/MyDrive/wheat_merged/val", "annotations"), exist_ok=True)
os.makedirs(os.path.join("/content/drive/MyDrive/wheat_merged/test", "annotations"), exist_ok=True)


# Load your annotated dataset (assuming COCO format JSON file)
with open('/content/drive/MyDrive/wheat_merged/annotations/instances.json', 'r') as f:
    coco_data = json.load(f)

# Step 1: Split the data into train + val and test sets
train_val_images, test_images = train_test_split(coco_data["images"], test_size=0.2, random_state=42)

# Step 2: Split the train + val set further into train and val
train_images, val_images = train_test_split(train_val_images, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2

# Step 3: Create annotations for each split
train_annotations = [ann for ann in coco_data["annotations"] if ann["image_id"] in {img["id"] for img in train_images}]
val_annotations = [ann for ann in coco_data["annotations"] if ann["image_id"] in {img["id"] for img in val_images}]
test_annotations = [ann for ann in coco_data["annotations"] if ann["image_id"] in {img["id"] for img in test_images}]

# Step 4: Save the split datasets into new JSON files
train_coco_data = {"images": train_images, "annotations": train_annotations, "categories": coco_data["categories"]}
val_coco_data = {"images": val_images, "annotations": val_annotations, "categories": coco_data["categories"]}
test_coco_data = {"images": test_images, "annotations": test_annotations, "categories": coco_data["categories"]}

# Save train split
with open('/content/drive/MyDrive/wheat_merged/train/annotations/instances.json', 'w') as f:
    json.dump(train_coco_data, f)

# Save val split
with open('/content/drive/MyDrive/wheat_merged/val/annotations/instances.json', 'w') as f:
    json.dump(val_coco_data, f)

# Save test split
with open('/content/drive/MyDrive/wheat_merged/test/annotations/instances.json', 'w') as f:
    json.dump(test_coco_data, f)

# Step 5: Copy images into respective train, val, and test folders
# Helper function to copy images
def copy_images(image_list, destination_folder, source_folder):
    for img in image_list:
        img_filename = img["file_name"]
        src_path = os.path.join(source_folder, img_filename)
        dst_path = os.path.join(destination_folder, img_filename)
        if os.path.exists(src_path):  # Check if the image exists in source folder
            shutil.copy(src_path, dst_path)
        else:
            print(f"Warning: File {img_filename} not found in source folder.")

# Copy images to respective folders
copy_images(train_images, train_images_folder, source_images_path)
copy_images(val_images, val_images_folder, source_images_path)
copy_images(test_images, test_images_folder, source_images_path)

print("Training, validation, and test datasets created with images organized into respective folders.")

"""# convert train json to label"""



# dont run again
import json
import os

# Paths
json_file_path = "/content/drive/MyDrive/wheat_merged/train/annotations/instances.json"
output_labels_dir = "/content/drive/MyDrive/wheat_merged/train/labels"
os.makedirs(output_labels_dir, exist_ok=True)  # Ensure output directory exists

# Load JSON
with open(json_file_path, "r") as f:
    data = json.load(f)

# Build a map for category IDs to indices
categories = {cat['id']: idx for idx, cat in enumerate(data['categories'])}

# Iterate through images and annotations
images = {img['id']: img for img in data['images']}
annotations = data['annotations']

# Group annotations by image_id
annotations_by_image = {}
for ann in annotations:
    img_id = ann['image_id']
    if img_id not in annotations_by_image:
        annotations_by_image[img_id] = []
    annotations_by_image[img_id].append(ann)

# Process each image
for img_id, img_data in images.items():
    img_annotations = annotations_by_image.get(img_id, [])
    img_width = img_data['width']
    img_height = img_data['height']

    label_lines = []

    # Process each annotation
    for ann in img_annotations:
        category_id = ann['category_id']
        bbox = ann['bbox']  # [x_min, y_min, width, height]

        # Convert bbox to YOLO format
        x_min, y_min, bbox_width, bbox_height = bbox
        x_center = (x_min + bbox_width / 2) / img_width
        y_center = (y_min + bbox_height / 2) / img_height
        norm_width = bbox_width / img_width
        norm_height = bbox_height / img_height

        # Prepare YOLO line
        label_line = f"{categories[category_id]} {x_center} {y_center} {norm_width} {norm_height}"
        label_lines.append(label_line)

    # Save to label file
    label_file_name = os.path.splitext(img_data['file_name'])[0] + ".txt"
    label_file_path = os.path.join(output_labels_dir, label_file_name)
    with open(label_file_path, "w") as label_file:
        label_file.write("\n".join(label_lines))

print("Conversion complete. Labels saved to:", output_labels_dir)

"""# test json to label"""

# dont run again
import json
import os

# Paths
json_file_path = "/content/drive/MyDrive/wheat_merged/test/annotations/instances.json"
output_labels_dir = "/content/drive/MyDrive/wheat_merged/test/labels"
os.makedirs(output_labels_dir, exist_ok=True)  # Ensure output directory exists

# Load JSON
with open(json_file_path, "r") as f:
    data = json.load(f)

# Build a map for category IDs to indices
categories = {cat['id']: idx for idx, cat in enumerate(data['categories'])}

# Iterate through images and annotations
images = {img['id']: img for img in data['images']}
annotations = data['annotations']

# Group annotations by image_id
annotations_by_image = {}
for ann in annotations:
    img_id = ann['image_id']
    if img_id not in annotations_by_image:
        annotations_by_image[img_id] = []
    annotations_by_image[img_id].append(ann)

# Process each image
for img_id, img_data in images.items():
    img_annotations = annotations_by_image.get(img_id, [])
    img_width = img_data['width']
    img_height = img_data['height']

    label_lines = []

    # Process each annotation
    for ann in img_annotations:
        category_id = ann['category_id']
        bbox = ann['bbox']  # [x_min, y_min, width, height]

        # Convert bbox to YOLO format
        x_min, y_min, bbox_width, bbox_height = bbox
        x_center = (x_min + bbox_width / 2) / img_width
        y_center = (y_min + bbox_height / 2) / img_height
        norm_width = bbox_width / img_width
        norm_height = bbox_height / img_height

        # Prepare YOLO line
        label_line = f"{categories[category_id]} {x_center} {y_center} {norm_width} {norm_height}"
        label_lines.append(label_line)

    # Save to label file
    label_file_name = os.path.splitext(img_data['file_name'])[0] + ".txt"
    label_file_path = os.path.join(output_labels_dir, label_file_name)
    with open(label_file_path, "w") as label_file:
        label_file.write("\n".join(label_lines))

print("Conversion complete. Labels saved to:", output_labels_dir)

"""# val json to label"""

# dont run again
import json
import os

# Paths
json_file_path = "/content/drive/MyDrive/wheat_merged/val/annotations/instances.json"
output_labels_dir = "/content/drive/MyDrive/wheat_merged/val/labels"
os.makedirs(output_labels_dir, exist_ok=True)  # Ensure output directory exists

# Load JSON
with open(json_file_path, "r") as f:
    data = json.load(f)

# Build a map for category IDs to indices
categories = {cat['id']: idx for idx, cat in enumerate(data['categories'])}

# Iterate through images and annotations
images = {img['id']: img for img in data['images']}
annotations = data['annotations']

# Group annotations by image_id
annotations_by_image = {}
for ann in annotations:
    img_id = ann['image_id']
    if img_id not in annotations_by_image:
        annotations_by_image[img_id] = []
    annotations_by_image[img_id].append(ann)

# Process each image
for img_id, img_data in images.items():
    img_annotations = annotations_by_image.get(img_id, [])
    img_width = img_data['width']
    img_height = img_data['height']

    label_lines = []

    # Process each annotation
    for ann in img_annotations:
        category_id = ann['category_id']
        bbox = ann['bbox']  # [x_min, y_min, width, height]

        # Convert bbox to YOLO format
        x_min, y_min, bbox_width, bbox_height = bbox
        x_center = (x_min + bbox_width / 2) / img_width
        y_center = (y_min + bbox_height / 2) / img_height
        norm_width = bbox_width / img_width
        norm_height = bbox_height / img_height

        # Prepare YOLO line
        label_line = f"{categories[category_id]} {x_center} {y_center} {norm_width} {norm_height}"
        label_lines.append(label_line)

    # Save to label file
    label_file_name = os.path.splitext(img_data['file_name'])[0] + ".txt"
    label_file_path = os.path.join(output_labels_dir, label_file_name)
    with open(label_file_path, "w") as label_file:
        label_file.write("\n".join(label_lines))

print("Conversion complete. Labels saved to:", output_labels_dir)

import cv2
from google.colab.patches import cv2_imshow # Import cv2_imshow

def draw_yolo_boxes(image_path, label_path, class_names):
    image = cv2.imread(image_path)
    height, width = image.shape[:2]

    with open(label_path, "r") as f:
        for line in f:
            data = line.split()
            class_id = int(data[0])
            x_center, y_center, box_width, box_height = map(float, data[1:])

            # Convert to absolute coordinates
            x_min = int((x_center - box_width / 2) * width)
            y_min = int((y_center - box_height / 2) * height)
            x_max = int((x_center + box_width / 2) * width)
            y_max = int((y_center + box_height / 2) * height)

            # Draw the box
            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
            cv2.putText(image, class_names[class_id], (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    cv2_imshow(image) # Use cv2_imshow instead of cv2.imshow
    cv2.waitKey(0)
    cv2.destroyAllWindows()

# Example usage
draw_yolo_boxes("/content/drive/MyDrive/wheat_merged/train/images/SKM_C55824101110460.jpg", "/content/drive/MyDrive/wheat_merged/train/labels/SKM_C55824101110460.txt",["sorghum", "dockage", "wheat", "broken_wheat"])

!pip install supervision

# dont run again
yaml_content = """
train: /content/drive/MyDrive/wheat_merged/train/images
val: /content/drive/MyDrive/wheat_merged/val/images
test: /content/drive/MyDrive/wheat_merged/test/images

nc: 4  # Number of classes (update with your number of classes)
names: ["sorghum", "dockage", "wheat", "broken_wheat"]  # Replace with your class names
"""

with open('/content/drive/MyDrive/wheat_merged/wheat.yaml', 'w') as f:
    f.write(yaml_content)

!pip install ultralytics

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install matplotlib

from ultralytics import YOLO
import matplotlib.pyplot as plt

# Load a model
model = YOLO("/content/drive/MyDrive/wheat_merged/yolo11m.pt")

# Train the model, saving results to a file
results = model.train(
    data="/content/drive/MyDrive/wheat_merged/wheat.yaml",  # dataset YAML
    epochs=200,  # number of epochs
    imgsz=640,  # image size
    batch=4,  # batch size
    device=0,  # GPU or CPU
    project="/content/drive/MyDrive/wheat_merged", # Save results to this folder
    name="Yolo11m_metrics_200", # Save results to this folder
)



# Evaluate the model: Validation
metrics = model.val()
print(metrics)

# Evaluate the model on the test set
test_metrics = model.val(data="/content/drive/MyDrive/wheat_merged/wheat.yaml", split='test')
print(test_metrics)

import numpy as np
confusion_matrix = np.array([
    [89, 0, 0, 0, 1],   # Sorghum
    [0, 74, 0, 0, 10],    # Dockage
    [0, 0, 491, 7, 4], # Wheat
    [0, 0, 0, 78, 1],    # Broken_Wheat
    [0, 1, 11, 6, 0]     # Background
])

# Initialize precision and recall lists
precision = []
recall = []

# Number of classes
num_classes = confusion_matrix.shape[0]

for i in range(num_classes):
    tp = confusion_matrix[i, i]  # True Positives
    fp = np.sum(confusion_matrix[:, i]) - tp  # False Positives
    fn = np.sum(confusion_matrix[i, :]) - tp  # False Negatives

    precision_value = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall_value = tp / (tp + fn) if (tp + fn) > 0 else 0

    precision.append(precision_value)
    recall.append(recall_value)

# Print results
classes = ["Sorghum", "Dockage", "Wheat", "Broken_Wheat"]
for idx, class_name in enumerate(classes):
    print(f"{class_name} - Precision: {precision[idx]:.2f}, Recall: {recall[idx]:.2f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

# Corrected confusion matrix
data = np.array([
    [89, 0, 0, 0, 1],   # Sorghum
    [0, 74, 0, 0, 10],  # Dockage
    [0, 0, 491, 7, 4],  # Wheat
    [0, 0, 0, 78, 1],   # Broken_Wheat
    [0, 1, 11, 6, 0]    # Background
])

# Class labels
class_labels = ["Sorghum", "Dockage", "Wheat", "Broken_Wheat", "Background"]

# Plot confusion matrix
fig, ax = plt.subplots(figsize=(8, 8))
display = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=class_labels)
display.plot(ax=ax, cmap="Blues", colorbar=True)

# Customize plot
plt.title("Confusion Matrix")
plt.xticks(rotation=45)
plt.show()

import locale
import os
from ultralytics import YOLO
import matplotlib.pyplot as plt

# Fix locale encoding
def getpreferredencoding(do_setlocale=True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

# Load the saved YOLO model
model = YOLO("/content/drive/MyDrive/wheat_merged/yolo11m.pt")

# Path to the folder containing test images
test_images_folder = "/content/drive/MyDrive/wheat_merged/test/images"

# Initialize a dictionary to store the object counts
object_counts = {}

# Loop through each image in the test folder
for image_name in os.listdir(test_images_folder):
    if image_name.endswith((".jpg", ".png", ".jpeg")):  # Adjust based on image formats
        image_path = os.path.join(test_images_folder, image_name)

        # Run prediction on the image
        results = model(image_path)

        # Count the number of predicted objects
        num_objects = len(results[0].boxes)  # `results[0].boxes` contains the bounding boxes for the objects

        # Save the count in the dictionary
        object_counts[image_name] = num_objects

# Print the results
for image_name, count in object_counts.items():
    print(f"Image: {image_name}, Predicted Objects: {count}")

# Optionally, save the counts to a file
output_file = "/content/drive/MyDrive/wheat_merged/test_object_counts.txt"
with open(output_file, "w") as f:
    for image_name, count in object_counts.items():
        f.write(f"Image: {image_name}, Predicted Objects: {count}\n")

print(f"Object counts saved to {output_file}")